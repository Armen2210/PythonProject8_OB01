#import requests
#from bs4 import BeautifulSoup
import time
import csv
from selenium import webdriver
#from selenium.webdriver import Keys
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException

# url = "https://quotes.toscrape.com/"
#
# response = requests.get(url)
#
# soup = BeautifulSoup(response.text, "html.parser")
#
# rows = soup.find_all("tr") # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è "find_all", —Ç—ç–≥ "tr" –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä—è–¥–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü–µ
# data = []
#
# for row in rows:
#     cols = row.find_all("td") #td —ç—Ç–æ –∫–æ–ª–æ–Ω–∫–∏
#     cleaned_cols = [col.text.strip() for col in cols] #strip() —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã; –≤ —Å–∫–æ–±–∫–∞—Ö –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ —É–¥–∞–ª—è—Ç—å
#     data.append(cleaned_cols)
#
# print(data)


# data = [
#     ['100', '200', '300'],
#     ['400', '500', '600']
#     ]

# numbers = []
#
# for row in data:
#     for text in row:
#         number = int(text)
#         numbers.append(number)
# print(numbers)


# list = []
#
# data = [
#     [100, 110, 120],
#     [400, 500, 600],
#     [150, 130, 140]
#     ]
#
# for row in data:
#     for item in row:
#         if item > 190:
#             list.append(item)
# print(list)

driver = webdriver.Chrome()
url  = "https://tomsk.hh.ru/vacancies/programmist"

driver.get(url)
time.sleep(5)

vacancies = driver.find_elements(By.CLASS_NAME, 'vacancy-card--n77Dj8TY8VIUF0yM')
print(f"[INFO] –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –≤–∞–∫–∞–Ω—Å–∏–π: {len(vacancies)}")

parsed_data = []

for index, vacancy in enumerate(vacancies, start=1):
    try:
        print(f"\n[INFO] –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ ‚Ññ{index}")

        try:
            title_element = vacancy.find_element(By.CSS_SELECTOR, 'span.magritte-text___tkzIl_6-0-13')
            title = title_element.text
            link = title_element.get_attribute('href')
        except NoSuchElementException:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏")
            title = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
            link = "–ù–µ—Ç —Å—Å—ã–ª–∫–∏"

        try:
            company = vacancy.find_element(By.CSS_SELECTOR, 'span.magritte-text___tkzIl_6-0-13').text
        except NoSuchElementException:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–º–ø–∞–Ω–∏—è")
            company = "–ù–µ —É–∫–∞–∑–∞–Ω–æ"

        try:
            salary = vacancy.find_element(By.CSS_SELECTOR, 'div.narrow-container--HaV4hduxPuElpx0V').text
        except NoSuchElementException:
            print("‚ö†Ô∏è –ó–∞—Ä–ø–ª–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            salary = "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

        try:
            link = vacancy.find_element(By.CSS_SELECTOR, 'a.magritte-link___b4rEM_6-0-13').get_attribute('href')
        except NoSuchElementException:
            print("‚ö†Ô∏è –°—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            link = "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

        parsed_data.append([title, company, salary, link])
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {title} ‚Äî {company} ‚Äî {salary} - {link}")

    except Exception as e:
        print(f"üö® –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –≤–∞–∫–∞–Ω—Å–∏–∏ ‚Ññ{index}: {e}")
        continue

with open('hh.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏', '–ö–æ–º–ø–∞–Ω–∏—è', '–ó–∞—Ä–ø–ª–∞—Ç–∞', '–°—Å—ã–ª–∫–∞'])
    writer.writerows(parsed_data)

print(f"\n[INFO] –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(parsed_data)} –∑–∞–ø–∏—Å–µ–π –≤ hh.csv")

driver.quit()




